{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:50:39.873067Z","iopub.execute_input":"2023-11-05T07:50:39.873372Z","iopub.status.idle":"2023-11-05T07:50:52.468843Z","shell.execute_reply.started":"2023-11-05T07:50:39.873347Z","shell.execute_reply":"2023-11-05T07:50:52.467808Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nRobertatokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nBioclinicaltokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:50:52.470765Z","iopub.execute_input":"2023-11-05T07:50:52.471088Z","iopub.status.idle":"2023-11-05T07:51:07.877749Z","shell.execute_reply.started":"2023-11-05T07:50:52.471058Z","shell.execute_reply":"2023-11-05T07:51:07.876974Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32bb735280c248bab3729db723f8659a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cebd7d061e1b4322b072fc76d41784d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a79bcd6cbb334353b503626747ed24ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a57a5b5f3b4c098d3ae016fb5231e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0d0ae97e40407289e6328d1107d27b"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Setting the path to files and model name","metadata":{}},{"cell_type":"code","source":"model_name = 'roberta' # can be 'bioclinical' for bioclinical_bert or 'roberta' for Roberta\ntrain_path = \"/kaggle/input/ihqid-1mg/IHQID-1mg/train.csv\"\ntest_path = \"/kaggle/input/ihqid-1mg/IHQID-1mg/test.csv\"\n\nMAX_LEN = 200\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 10\nLEARNING_RATE = 1e-05","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:07.879034Z","iopub.execute_input":"2023-11-05T07:51:07.879304Z","iopub.status.idle":"2023-11-05T07:51:07.884191Z","shell.execute_reply.started":"2023-11-05T07:51:07.879277Z","shell.execute_reply":"2023-11-05T07:51:07.883207Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Loading and Preprocessing Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:07.886207Z","iopub.execute_input":"2023-11-05T07:51:07.886491Z","iopub.status.idle":"2023-11-05T07:51:07.966131Z","shell.execute_reply.started":"2023-11-05T07:51:07.886466Z","shell.execute_reply":"2023-11-05T07:51:07.965190Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def one_hot_encode(row):\n    one_hot = [1 if cell else 0 for cell in row]\n    return one_hot\n\ntrain_eng = train_df[['question_english', 'Manual_Intent']]\ntest_eng = test_df[['question_english', 'Manual_Intent']]\n\nunique_values = train_eng['Manual_Intent'].unique()\n\n# Function to create the one-hot encoded list for each row\ndef one_hot_encode_category(row, unique_values):\n    one_hot = [1 if value == row['Manual_Intent'] else 0 for value in unique_values]\n    return one_hot\n\n# Apply the function to create the new one-hot encoded column\ntrain_eng['one_hot_encoded'] = train_eng.apply(one_hot_encode_category, args=(unique_values,), axis=1)\ntest_eng['one_hot_encoded'] = test_eng.apply(one_hot_encode_category, args=(unique_values,), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:07.967334Z","iopub.execute_input":"2023-11-05T07:51:07.967691Z","iopub.status.idle":"2023-11-05T07:51:08.001443Z","shell.execute_reply.started":"2023-11-05T07:51:07.967656Z","shell.execute_reply":"2023-11-05T07:51:08.000590Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_eng = train_eng[['question_english', 'one_hot_encoded']]\ntest_eng = test_eng[['question_english', 'one_hot_encoded']]\ntest_eng.columns = ['input', 'target']\ntrain_eng.columns = ['input', 'target']","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:08.002466Z","iopub.execute_input":"2023-11-05T07:51:08.002787Z","iopub.status.idle":"2023-11-05T07:51:08.013905Z","shell.execute_reply.started":"2023-11-05T07:51:08.002762Z","shell.execute_reply":"2023-11-05T07:51:08.013126Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.input = dataframe.input\n        self.targets = self.data.target\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.input)\n\n    def __getitem__(self, index):\n        text = str(self.input[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:08.014979Z","iopub.execute_input":"2023-11-05T07:51:08.015356Z","iopub.status.idle":"2023-11-05T07:51:08.024881Z","shell.execute_reply.started":"2023-11-05T07:51:08.015331Z","shell.execute_reply":"2023-11-05T07:51:08.024175Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_eng\ntest_dataset = test_eng\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\nif(model_name == 'roberta'):\n    tokenizer = Robertatokenizer\nelif(model_name == 'bioclinical'):\n    tokenizer = Bioclinicaltokenizer\nelse:\n    print(\"Doesnt exist model name, please enter correctly\")\n    \ntraining_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:08.025907Z","iopub.execute_input":"2023-11-05T07:51:08.026225Z","iopub.status.idle":"2023-11-05T07:51:08.040519Z","shell.execute_reply.started":"2023-11-05T07:51:08.026188Z","shell.execute_reply":"2023-11-05T07:51:08.039717Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"TRAIN Dataset: (305, 2)\nTEST Dataset: (112, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,'shuffle': True,'num_workers': 0}\ntest_params = {'batch_size': VALID_BATCH_SIZE,'shuffle': True,'num_workers': 0}\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:08.041572Z","iopub.execute_input":"2023-11-05T07:51:08.042147Z","iopub.status.idle":"2023-11-05T07:51:08.050836Z","shell.execute_reply.started":"2023-11-05T07:51:08.042115Z","shell.execute_reply":"2023-11-05T07:51:08.050056Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Building Customized classes for model for end to end finetuning","metadata":{}},{"cell_type":"code","source":" class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained('roberta-base')\n        self.l2 = torch.nn.Linear(768, 4)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n        output = self.l2(output_1)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:08.053487Z","iopub.execute_input":"2023-11-05T07:51:08.053784Z","iopub.status.idle":"2023-11-05T07:51:08.062873Z","shell.execute_reply.started":"2023-11-05T07:51:08.053753Z","shell.execute_reply":"2023-11-05T07:51:08.062028Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":" class BioclinicalClass(torch.nn.Module):\n    def __init__(self):\n        super(BioclinicalClass, self).__init__()\n        self.l1 = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n        self.l2 = torch.nn.Linear(768, 4)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n        output = self.l2(output_1)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:08.063985Z","iopub.execute_input":"2023-11-05T07:51:08.064305Z","iopub.status.idle":"2023-11-05T07:51:08.075715Z","shell.execute_reply.started":"2023-11-05T07:51:08.064271Z","shell.execute_reply":"2023-11-05T07:51:08.074953Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Initializing the model and setting the loss function and optimizer","metadata":{}},{"cell_type":"code","source":"if(model_name == 'roberta'):\n    model = RobertaClass()\nelif (model_name == 'bioclinical'):\n    model = BioclinicalClass()\n    \nmodel.to(device)\ndef loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:08.076617Z","iopub.execute_input":"2023-11-05T07:51:08.076891Z","iopub.status.idle":"2023-11-05T07:51:16.724876Z","shell.execute_reply.started":"2023-11-05T07:51:08.076867Z","shell.execute_reply":"2023-11-05T07:51:16.724069Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c98cb435be847aba62ce3408e1366a7"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train(epoch):\n    model.train()\n    for _,data in enumerate(training_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n        outputs = model(ids, mask, token_type_ids)\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets.float())\n        if _%5000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:16.726067Z","iopub.execute_input":"2023-11-05T07:51:16.726362Z","iopub.status.idle":"2023-11-05T07:51:16.733325Z","shell.execute_reply.started":"2023-11-05T07:51:16.726335Z","shell.execute_reply":"2023-11-05T07:51:16.732482Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:51:16.734573Z","iopub.execute_input":"2023-11-05T07:51:16.734903Z","iopub.status.idle":"2023-11-05T07:52:22.831909Z","shell.execute_reply.started":"2023-11-05T07:51:16.734871Z","shell.execute_reply":"2023-11-05T07:52:22.831137Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss:  0.7172093391418457\nEpoch: 1, Loss:  0.5318869352340698\nEpoch: 2, Loss:  0.48000818490982056\nEpoch: 3, Loss:  0.32376354932785034\nEpoch: 4, Loss:  0.11979366838932037\nEpoch: 5, Loss:  0.0647515058517456\nEpoch: 6, Loss:  0.10024773329496384\nEpoch: 7, Loss:  0.06543572247028351\nEpoch: 8, Loss:  0.048537008464336395\nEpoch: 9, Loss:  0.05464068427681923\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def test_validation():\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask, token_type_ids)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:52:22.833424Z","iopub.execute_input":"2023-11-05T07:52:22.833815Z","iopub.status.idle":"2023-11-05T07:52:22.841370Z","shell.execute_reply.started":"2023-11-05T07:52:22.833775Z","shell.execute_reply":"2023-11-05T07:52:22.840452Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"outputs, targets = test_validation()\noutputs = np.array(outputs) >= 0.5\nf1_score_macro = metrics.f1_score(targets, outputs, average='macro')\nclassification_report = metrics.classification_report(targets, outputs)\nconfusion_matrix = metrics.multilabel_confusion_matrix(targets, outputs)\nprint(f\"Test F1 Score (Macro) = {f1_score_macro}\")\nprint(\"Test classification report = \\n\")\nprint(classification_report)\nprint(\"Test confusion matrix = \\n\")\nprint(confusion_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:52:22.842536Z","iopub.execute_input":"2023-11-05T07:52:22.842811Z","iopub.status.idle":"2023-11-05T07:52:23.745232Z","shell.execute_reply.started":"2023-11-05T07:52:22.842787Z","shell.execute_reply":"2023-11-05T07:52:23.744347Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Test F1 Score (Macro) = 0.6812643185929343\nTest classification report = \n\n              precision    recall  f1-score   support\n\n           0       0.90      1.00      0.95        54\n           1       0.29      0.31      0.30        13\n           2       0.76      0.79      0.78        24\n           3       0.92      0.57      0.71        21\n\n   micro avg       0.79      0.79      0.79       112\n   macro avg       0.72      0.67      0.68       112\nweighted avg       0.80      0.79      0.79       112\n samples avg       0.78      0.79      0.79       112\n\nTest confusion matrix = \n\n[[[52  6]\n  [ 0 54]]\n\n [[89 10]\n  [ 9  4]]\n\n [[82  6]\n  [ 5 19]]\n\n [[90  1]\n  [ 9 12]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}