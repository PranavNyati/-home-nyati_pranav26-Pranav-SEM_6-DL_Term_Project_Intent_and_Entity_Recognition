{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:13:39.446301Z","iopub.execute_input":"2023-11-05T07:13:39.446647Z","iopub.status.idle":"2023-11-05T07:13:52.130116Z","shell.execute_reply.started":"2023-11-05T07:13:39.446614Z","shell.execute_reply":"2023-11-05T07:13:52.128879Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nRobertatokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nBioclinicaltokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:13:52.132466Z","iopub.execute_input":"2023-11-05T07:13:52.132831Z","iopub.status.idle":"2023-11-05T07:14:07.073350Z","shell.execute_reply.started":"2023-11-05T07:13:52.132801Z","shell.execute_reply":"2023-11-05T07:14:07.072581Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bc029f9a6114ceabafb0e0f97237946"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d862efdd52f9481883dda04b61efd0ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac3bc562190f4846ab00c987743d59e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed22b57d3f84ee2b855322531bdc176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae26f23971b4b4b94eb22a1fa8a0d0f"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Setting the path to files and model name","metadata":{}},{"cell_type":"code","source":"model_name = 'bioclinical' # can be 'bioclinical' for bioclinical_bert or 'roberta' for Roberta\ntrain_path = \"/kaggle/input/ihqid-1mg/IHQID-1mg/train.csv\"\ntest_path = \"/kaggle/input/ihqid-1mg/IHQID-1mg/test.csv\"\n\nMAX_LEN = 200\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 10\nLEARNING_RATE = 1e-05","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.074355Z","iopub.execute_input":"2023-11-05T07:14:07.074632Z","iopub.status.idle":"2023-11-05T07:14:07.079460Z","shell.execute_reply.started":"2023-11-05T07:14:07.074604Z","shell.execute_reply":"2023-11-05T07:14:07.078588Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Loading and Preprocessing Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.082049Z","iopub.execute_input":"2023-11-05T07:14:07.082877Z","iopub.status.idle":"2023-11-05T07:14:07.150906Z","shell.execute_reply.started":"2023-11-05T07:14:07.082819Z","shell.execute_reply":"2023-11-05T07:14:07.149919Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def one_hot_encode(row):\n    one_hot = [1 if cell else 0 for cell in row]\n    return one_hot\n\ntrain_eng = train_df[['question_english', 'Manual_Intent']]\ntest_eng = test_df[['question_english', 'Manual_Intent']]\n\nunique_values = train_eng['Manual_Intent'].unique()\n\n# Function to create the one-hot encoded list for each row\ndef one_hot_encode_category(row, unique_values):\n    one_hot = [1 if value == row['Manual_Intent'] else 0 for value in unique_values]\n    return one_hot\n\n# Apply the function to create the new one-hot encoded column\ntrain_eng['one_hot_encoded'] = train_eng.apply(one_hot_encode_category, args=(unique_values,), axis=1)\ntest_eng['one_hot_encoded'] = test_eng.apply(one_hot_encode_category, args=(unique_values,), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.152075Z","iopub.execute_input":"2023-11-05T07:14:07.152498Z","iopub.status.idle":"2023-11-05T07:14:07.186636Z","shell.execute_reply.started":"2023-11-05T07:14:07.152393Z","shell.execute_reply":"2023-11-05T07:14:07.185864Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_eng = train_eng[['question_english', 'one_hot_encoded']]\ntest_eng = test_eng[['question_english', 'one_hot_encoded']]\ntest_eng.columns = ['input', 'target']\ntrain_eng.columns = ['input', 'target']","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.187560Z","iopub.execute_input":"2023-11-05T07:14:07.187837Z","iopub.status.idle":"2023-11-05T07:14:07.194085Z","shell.execute_reply.started":"2023-11-05T07:14:07.187815Z","shell.execute_reply":"2023-11-05T07:14:07.193102Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.input = dataframe.input\n        self.targets = self.data.target\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.input)\n\n    def __getitem__(self, index):\n        text = str(self.input[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.195126Z","iopub.execute_input":"2023-11-05T07:14:07.195399Z","iopub.status.idle":"2023-11-05T07:14:07.208480Z","shell.execute_reply.started":"2023-11-05T07:14:07.195376Z","shell.execute_reply":"2023-11-05T07:14:07.207620Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_eng\ntest_dataset = test_eng\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\nif(model_name == 'roberta'):\n    tokenizer = Robertatokenizer\nelif(model_name == 'bioclinical'):\n    tokenizer = Bioclinicaltokenizer\nelse:\n    print(\"Doesnt exist model name, please enter correctly\")\n    \ntraining_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.209609Z","iopub.execute_input":"2023-11-05T07:14:07.209931Z","iopub.status.idle":"2023-11-05T07:14:07.225036Z","shell.execute_reply.started":"2023-11-05T07:14:07.209900Z","shell.execute_reply":"2023-11-05T07:14:07.224152Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"TRAIN Dataset: (305, 2)\nTEST Dataset: (112, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,'shuffle': True,'num_workers': 0}\ntest_params = {'batch_size': VALID_BATCH_SIZE,'shuffle': True,'num_workers': 0}\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.226125Z","iopub.execute_input":"2023-11-05T07:14:07.226466Z","iopub.status.idle":"2023-11-05T07:14:07.236875Z","shell.execute_reply.started":"2023-11-05T07:14:07.226442Z","shell.execute_reply":"2023-11-05T07:14:07.236004Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Building Customized classes for model for end to end finetuning","metadata":{}},{"cell_type":"code","source":" class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained('roberta-base')\n        self.l2 = torch.nn.Linear(768, 4)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n        output = self.l2(output_1)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.239683Z","iopub.execute_input":"2023-11-05T07:14:07.239978Z","iopub.status.idle":"2023-11-05T07:14:07.249350Z","shell.execute_reply.started":"2023-11-05T07:14:07.239955Z","shell.execute_reply":"2023-11-05T07:14:07.248564Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":" class BioclinicalClass(torch.nn.Module):\n    def __init__(self):\n        super(BioclinicalClass, self).__init__()\n        self.l1 = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n        self.l2 = torch.nn.Linear(768, 4)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n        output = self.l2(output_1)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.250459Z","iopub.execute_input":"2023-11-05T07:14:07.251084Z","iopub.status.idle":"2023-11-05T07:14:07.265243Z","shell.execute_reply.started":"2023-11-05T07:14:07.251058Z","shell.execute_reply":"2023-11-05T07:14:07.264529Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Initializing the model and setting the loss function and optimizer","metadata":{}},{"cell_type":"code","source":"if(model_name == 'roberta'):\n    model = RobertaClass()\nelif (model_name == 'bioclinical'):\n    model = BioclinicalClass()\n    \nmodel.to(device)\ndef loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:07.266425Z","iopub.execute_input":"2023-11-05T07:14:07.266891Z","iopub.status.idle":"2023-11-05T07:14:15.871769Z","shell.execute_reply.started":"2023-11-05T07:14:07.266860Z","shell.execute_reply":"2023-11-05T07:14:15.870779Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"582462b1e8084cf3881d1f6467ed29b8"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train(epoch):\n    model.train()\n    for _,data in enumerate(training_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n        outputs = model(ids, mask, token_type_ids)\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets.float())\n        if _%5000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:15.873163Z","iopub.execute_input":"2023-11-05T07:14:15.873838Z","iopub.status.idle":"2023-11-05T07:14:15.881457Z","shell.execute_reply.started":"2023-11-05T07:14:15.873801Z","shell.execute_reply":"2023-11-05T07:14:15.880465Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:14:15.882746Z","iopub.execute_input":"2023-11-05T07:14:15.883235Z","iopub.status.idle":"2023-11-05T07:15:20.945577Z","shell.execute_reply.started":"2023-11-05T07:14:15.883199Z","shell.execute_reply":"2023-11-05T07:15:20.944772Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss:  0.7791520953178406\nEpoch: 1, Loss:  0.5376718640327454\nEpoch: 2, Loss:  0.47242727875709534\nEpoch: 3, Loss:  0.15439903736114502\nEpoch: 4, Loss:  0.23008888959884644\nEpoch: 5, Loss:  0.20785313844680786\nEpoch: 6, Loss:  0.19551409780979156\nEpoch: 7, Loss:  0.10811451077461243\nEpoch: 8, Loss:  0.10871139168739319\nEpoch: 9, Loss:  0.15547117590904236\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def test_validation():\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask, token_type_ids)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:15:20.946886Z","iopub.execute_input":"2023-11-05T07:15:20.947181Z","iopub.status.idle":"2023-11-05T07:15:20.954527Z","shell.execute_reply.started":"2023-11-05T07:15:20.947156Z","shell.execute_reply":"2023-11-05T07:15:20.953502Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"outputs, targets = test_validation()\noutputs = np.array(outputs) >= 0.5\nf1_score_macro = metrics.f1_score(targets, outputs, average='macro')\nclassification_report = metrics.classification_report(targets, outputs)\nconfusion_matrix = metrics.multilabel_confusion_matrix(targets, outputs)\nprint(f\"Test F1 Score (Macro) = {f1_score_macro}\")\nprint(\"Test classification report = \\n\")\nprint(classification_report)\nprint(\"Test confusion matrix = \\n\")\nprint(confusion_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:15:20.955740Z","iopub.execute_input":"2023-11-05T07:15:20.956008Z","iopub.status.idle":"2023-11-05T07:15:21.836981Z","shell.execute_reply.started":"2023-11-05T07:15:20.955985Z","shell.execute_reply":"2023-11-05T07:15:21.836061Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Test F1 Score (Macro) = 0.659098493039149\nTest classification report = \n\n              precision    recall  f1-score   support\n\n           0       0.90      0.98      0.94        54\n           1       0.50      0.15      0.24        13\n           2       0.70      0.79      0.75        24\n           3       0.78      0.67      0.72        21\n\n   micro avg       0.81      0.79      0.80       112\n   macro avg       0.72      0.65      0.66       112\nweighted avg       0.79      0.79      0.77       112\n samples avg       0.78      0.79      0.78       112\n\nTest confusion matrix = \n\n[[[52  6]\n  [ 1 53]]\n\n [[97  2]\n  [11  2]]\n\n [[80  8]\n  [ 5 19]]\n\n [[87  4]\n  [ 7 14]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}