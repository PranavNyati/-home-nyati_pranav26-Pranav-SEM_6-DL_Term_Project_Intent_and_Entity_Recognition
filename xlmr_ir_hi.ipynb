{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:17.719486Z","iopub.execute_input":"2023-11-10T22:14:17.719861Z","iopub.status.idle":"2023-11-10T22:14:32.124124Z","shell.execute_reply.started":"2023-11-10T22:14:17.719828Z","shell.execute_reply":"2023-11-10T22:14:32.122623Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:32.126395Z","iopub.execute_input":"2023-11-10T22:14:32.126771Z","iopub.status.idle":"2023-11-10T22:14:48.842872Z","shell.execute_reply.started":"2023-11-10T22:14:32.126737Z","shell.execute_reply":"2023-11-10T22:14:48.841827Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b07e8e01f17c41de9ce5efeeb157db69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bbe1a0b5b394801a7ff644ee54086e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dde7d4f61404471aeef0fd887a6db49"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Setting the path to files and model name","metadata":{}},{"cell_type":"code","source":"model_name = 'roberta' # can be 'bioclinical' for bioclinical_bert or 'roberta' for Roberta\ntrain_path = \"/kaggle/input/ihqid-webmd/IHQID-WebMD/train.csv\"\ntest_path = \"/kaggle/input/ihqid-webmd/IHQID-WebMD/test.csv\"\n\nMAX_LEN = 200\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 10\nLEARNING_RATE = 1e-05","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:48.844295Z","iopub.execute_input":"2023-11-10T22:14:48.845106Z","iopub.status.idle":"2023-11-10T22:14:48.850652Z","shell.execute_reply.started":"2023-11-10T22:14:48.845067Z","shell.execute_reply":"2023-11-10T22:14:48.849499Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Loading and Preprocessing Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:48.852972Z","iopub.execute_input":"2023-11-10T22:14:48.853272Z","iopub.status.idle":"2023-11-10T22:14:48.944499Z","shell.execute_reply.started":"2023-11-10T22:14:48.853246Z","shell.execute_reply":"2023-11-10T22:14:48.943608Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def one_hot_encode(row):\n    one_hot = [1 if cell else 0 for cell in row]\n    return one_hot\n\ntrain_eng = train_df[['question_hindi', 'Manual_Intent']]\ntest_eng = test_df[['question_hindi', 'Manual_Intent']]\n\nunique_values = train_eng['Manual_Intent'].unique()\n\n# Function to create the one-hot encoded list for each row\ndef one_hot_encode_category(row, unique_values):\n    one_hot = [1 if value == row['Manual_Intent'] else 0 for value in unique_values]\n    return one_hot\n\n# Apply the function to create the new one-hot encoded column\ntrain_eng['one_hot_encoded'] = train_eng.apply(one_hot_encode_category, args=(unique_values,), axis=1)\ntest_eng['one_hot_encoded'] = test_eng.apply(one_hot_encode_category, args=(unique_values,), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:48.946134Z","iopub.execute_input":"2023-11-10T22:14:48.946549Z","iopub.status.idle":"2023-11-10T22:14:49.001347Z","shell.execute_reply.started":"2023-11-10T22:14:48.946514Z","shell.execute_reply":"2023-11-10T22:14:49.000380Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_eng = train_eng[['question_hindi', 'one_hot_encoded']]\ntest_eng = test_eng[['question_hindi', 'one_hot_encoded']]\ntest_eng.columns = ['input', 'target']\ntrain_eng.columns = ['input', 'target']","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:49.002597Z","iopub.execute_input":"2023-11-10T22:14:49.002966Z","iopub.status.idle":"2023-11-10T22:14:49.012604Z","shell.execute_reply.started":"2023-11-10T22:14:49.002934Z","shell.execute_reply":"2023-11-10T22:14:49.011673Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.input = dataframe.input\n        self.targets = self.data.target\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.input)\n\n    def __getitem__(self, index):\n        text = str(self.input[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:49.013897Z","iopub.execute_input":"2023-11-10T22:14:49.015960Z","iopub.status.idle":"2023-11-10T22:14:49.026171Z","shell.execute_reply.started":"2023-11-10T22:14:49.015921Z","shell.execute_reply":"2023-11-10T22:14:49.025234Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_eng\ntest_dataset = test_eng\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n    \ntraining_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:49.027282Z","iopub.execute_input":"2023-11-10T22:14:49.027631Z","iopub.status.idle":"2023-11-10T22:14:49.041903Z","shell.execute_reply.started":"2023-11-10T22:14:49.027604Z","shell.execute_reply":"2023-11-10T22:14:49.040742Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"TRAIN Dataset: (720, 2)\nTEST Dataset: (241, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,'shuffle': True,'num_workers': 0}\ntest_params = {'batch_size': VALID_BATCH_SIZE,'shuffle': True,'num_workers': 0}\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:49.043417Z","iopub.execute_input":"2023-11-10T22:14:49.043740Z","iopub.status.idle":"2023-11-10T22:14:49.052871Z","shell.execute_reply.started":"2023-11-10T22:14:49.043713Z","shell.execute_reply":"2023-11-10T22:14:49.051907Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Building Customized classes for model for end to end finetuning","metadata":{}},{"cell_type":"code","source":" class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained('roberta-base')\n        self.l2 = torch.nn.Linear(768, 4)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n        output = self.l2(output_1)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:49.056873Z","iopub.execute_input":"2023-11-10T22:14:49.057210Z","iopub.status.idle":"2023-11-10T22:14:49.065801Z","shell.execute_reply.started":"2023-11-10T22:14:49.057177Z","shell.execute_reply":"2023-11-10T22:14:49.064980Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":" class BioclinicalClass(torch.nn.Module):\n    def __init__(self):\n        super(BioclinicalClass, self).__init__()\n        self.l1 = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n        self.l2 = torch.nn.Linear(768, 4)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n        output = self.l2(output_1)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:49.067076Z","iopub.execute_input":"2023-11-10T22:14:49.067934Z","iopub.status.idle":"2023-11-10T22:14:49.082787Z","shell.execute_reply.started":"2023-11-10T22:14:49.067901Z","shell.execute_reply":"2023-11-10T22:14:49.081893Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":" class XLMClass(torch.nn.Module):\n    def __init__(self):\n        super(XLMClass, self).__init__()\n        self.l1 = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n        self.l2 = torch.nn.Linear(250002, 4)\n    \n    def forward(self, ids, mask, token_type_ids):\n        output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict = False)\n        output = self.l2(output_1[0])\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:49.083971Z","iopub.execute_input":"2023-11-10T22:14:49.084326Z","iopub.status.idle":"2023-11-10T22:14:49.093910Z","shell.execute_reply.started":"2023-11-10T22:14:49.084298Z","shell.execute_reply":"2023-11-10T22:14:49.093090Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Initializing the model and setting the loss function and optimizer","metadata":{}},{"cell_type":"code","source":"model = XLMClass()    \nmodel.to(device)\ndef loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:14:49.095318Z","iopub.execute_input":"2023-11-10T22:14:49.095923Z","iopub.status.idle":"2023-11-10T22:15:22.474825Z","shell.execute_reply.started":"2023-11-10T22:14:49.095887Z","shell.execute_reply":"2023-11-10T22:15:22.473965Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc2e0c95d8c346498db2419a51ae8faf"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train(epoch):\n    model.train()\n    for _,data in enumerate(training_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n        outputs = model(ids, mask, token_type_ids)\n        outputs = outputs[:, 1] - outputs[:, 0]\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets.float())\n        if _%5000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:15:22.476061Z","iopub.execute_input":"2023-11-10T22:15:22.476395Z","iopub.status.idle":"2023-11-10T22:15:22.484238Z","shell.execute_reply.started":"2023-11-10T22:15:22.476368Z","shell.execute_reply":"2023-11-10T22:15:22.483257Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:15:22.485650Z","iopub.execute_input":"2023-11-10T22:15:22.486447Z","iopub.status.idle":"2023-11-10T22:22:32.622354Z","shell.execute_reply.started":"2023-11-10T22:15:22.486407Z","shell.execute_reply":"2023-11-10T22:22:32.620967Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss:  1.9325957298278809\nEpoch: 1, Loss:  0.5799528360366821\nEpoch: 2, Loss:  0.5265626907348633\nEpoch: 3, Loss:  0.37282299995422363\nEpoch: 4, Loss:  0.15848389267921448\nEpoch: 5, Loss:  0.4397836923599243\nEpoch: 6, Loss:  0.295495867729187\nEpoch: 7, Loss:  0.28135648369789124\nEpoch: 8, Loss:  0.2501368522644043\nEpoch: 9, Loss:  0.04627779871225357\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def test_validation():\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask, token_type_ids)\n            outputs = outputs[:, 1] - outputs[:, 0]\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:22:32.623996Z","iopub.execute_input":"2023-11-10T22:22:32.624351Z","iopub.status.idle":"2023-11-10T22:22:32.633760Z","shell.execute_reply.started":"2023-11-10T22:22:32.624322Z","shell.execute_reply":"2023-11-10T22:22:32.632597Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"outputs, targets = test_validation()\noutputs = np.array(outputs) >= 0.5\nf1_score_macro = metrics.f1_score(targets, outputs, average='macro')\nclassification_report = metrics.classification_report(targets, outputs)\nconfusion_matrix = metrics.multilabel_confusion_matrix(targets, outputs)\nprint(f\"Test F1 Score (Macro) = {f1_score_macro}\")\nprint(\"Test classification report = \\n\")\nprint(classification_report)\nprint(\"Test confusion matrix = \\n\")\nprint(confusion_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T22:22:32.634955Z","iopub.execute_input":"2023-11-10T22:22:32.635272Z","iopub.status.idle":"2023-11-10T22:22:40.159611Z","shell.execute_reply.started":"2023-11-10T22:22:32.635240Z","shell.execute_reply":"2023-11-10T22:22:40.158422Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Test F1 Score (Macro) = 0.6774783658144109\nTest classification report = \n\n              precision    recall  f1-score   support\n\n           0       0.75      0.74      0.74        53\n           1       0.78      0.67      0.72        73\n           2       0.86      0.55      0.67        76\n           3       0.49      0.69      0.57        39\n\n   micro avg       0.72      0.65      0.68       241\n   macro avg       0.72      0.66      0.68       241\nweighted avg       0.75      0.65      0.69       241\n samples avg       0.64      0.65      0.64       241\n\nTest confusion matrix = \n\n[[[175  13]\n  [ 14  39]]\n\n [[154  14]\n  [ 24  49]]\n\n [[158   7]\n  [ 34  42]]\n\n [[174  28]\n  [ 12  27]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}